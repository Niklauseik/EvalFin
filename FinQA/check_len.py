import json
import tiktoken

# é€‰æ‹©åˆé€‚çš„ tokenizerï¼ˆGPT-4o & GPT-3.5 ä½¿ç”¨çš„ BPEï¼‰
tokenizer = tiktoken.get_encoding("cl100k_base")

# ç›®æ ‡ JSONL æ–‡ä»¶è·¯å¾„
jsonl_files = [
    "datasets/finqa_jsonl/finqa_answer_first.jsonl",
    "datasets/finqa_jsonl/finqa_cot_first.jsonl"
]

TOKEN_LIMIT = 4096  # OpenAI å¾®è°ƒæœ€å¤§ token é™åˆ¶
token_counts = {}

for file_path in jsonl_files:
    with open(file_path, "r") as f:
        lines = f.readlines()
    
    tokens_per_sample = []
    for line in lines:
        data = json.loads(line)
        messages = data["messages"]
        
        # è®¡ç®—æ¯æ¡æ¶ˆæ¯çš„ token æ•°
        total_tokens = sum(len(tokenizer.encode(msg["content"])) for msg in messages)
        tokens_per_sample.append(total_tokens)

    # ç»Ÿè®¡ä¿¡æ¯
    max_tokens = max(tokens_per_sample)
    min_tokens = min(tokens_per_sample)
    avg_tokens = sum(tokens_per_sample) / len(tokens_per_sample)
    over_limit = sum(1 for x in tokens_per_sample if x > TOKEN_LIMIT)
    
    token_counts[file_path] = {
        "max_tokens": max_tokens,
        "min_tokens": min_tokens,
        "avg_tokens": avg_tokens,
        "total_samples": len(tokens_per_sample),
        "over_limit": over_limit,
        "over_limit_percentage": (over_limit / len(tokens_per_sample)) * 100
    }

# æ‰“å°ç»“æœ
for file, stats in token_counts.items():
    print(f"ğŸ“‚ æ–‡ä»¶: {file}")
    print(f"   - æœ€å¤§ token æ•°: {stats['max_tokens']}")
    print(f"   - æœ€å° token æ•°: {stats['min_tokens']}")
    print(f"   - å¹³å‡ token æ•°: {stats['avg_tokens']:.2f}")
    print(f"   - æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"   - è¶…è¿‡ {TOKEN_LIMIT} tokens çš„æ ·æœ¬æ•°: {stats['over_limit']} ({stats['over_limit_percentage']:.2f}%)\n")
